# -*- coding: utf-8 -*-
"""FAUP Day 3 HTML and Webscraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/116cd4qA9gIWj4xHTKFcRp7hXwtv5A_C2

Method to scrape data is via HTML code or through APIs.
Understat website right click and go to inspect to see html code and right click and go to view source code which is different from the HTML code.
ctrl + f = search in webpage.
"""

import json
import pandas as pd
from bs4 import BeautifulSoup
from urllib.request import urlopen

# Scrape top player stats:
scrape_url = "https://understat.com/league/EPL/2020"

urlopen(scrape_url) # It helps to establish connection between pyhton notebook and website we will use to scrape data.

page_connect = urlopen(scrape_url)

page_connect

page_html = BeautifulSoup(page_connect, "html.parser")

page_html

page_html.findAll(name = "script")

page_html.findAll(name = "script").__len__() # 26 codeblocks has name as script.

page_html.findAll(name="script")[3] # We have to look to each data by varyin index anf find our own data.

page_html.findAll(name="script")[3].text # return in form of unicode that is the computer understands.

# json is Java Script Object Notation in which data is tored in text format.
json_raw_string = page_html.findAll(name="script")[3].text

json_raw_string

"abc12324".index("24")

json_raw_string.index("\\")

json_raw_string.index("')")

start_ind = json_raw_string.index("\\")
stop_ind = json_raw_string.index("')")

json_raw_string[start_ind:stop_ind]

json_data = json_raw_string[start_ind:stop_ind]
json_data

json_data.encode("utf8") #UTF-8 is a variable-width character encoding used for electronic communication

json_data.encode("utf8").decode("unicode_escape")

json_data = json_data.encode("utf8").decode("unicode_escape")

json.loads(json_data)

json.loads(json_data).__len__()

final_json_df = pd.json_normalize(json.loads(json_data))

final_json_df

final_json_df.columns

final_json_df.columns.__len__()

final_json_df.info()

final_json_df.index

final_json_df.team_title.unique() # , between teams denote that one player is loan to other team

final_json_df.goals.unique()

b = [int(x) for x in final_json_df.goals.unique()] # converting string goals to int form.
b

sum(b) # prints total goals scored.

"""Scrape from the SofaScore website"""

import json
import requests # Requests is a HTTP library for the Python programming language. The goal of the project is to make HTTP requests simpler and more human-friendly.

scrape_url = "https://api.sofascore.com/api/v1/unique-tournament/1900/season/34713/standings/total"
r = requests.get(scrape_url) 
json_object = json.loads(r.content)

type(json_object)

json_object.__len__()

json_object.keys()

type(json_object["standings"])

json_object["standings"][0]

type(json_object["standings"][0])

# json_object["standings"][1] # Only 1 element so will thrwo error

json_object["standings"][0].keys()

json_object["standings"][0]["tournament"]

json_object["standings"][0]["rows"]

pd.json_normalize(json_object["standings"][0]["rows"])

isl_standings_df = pd.json_normalize(json_object["standings"][0]["rows"])

from google.colab import files
isl_standings_df.to_csv('isl_data work.csv')
files.download('isl_data work.csv')